{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shreyaschandrashekar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shreyaschandrashekar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # For TF-IDF feature extraction\n",
    "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes classifier\n",
    "from sklearn.model_selection import train_test_split  # Splitting data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 131\n",
      "Number of classes (intent tags): 27\n",
      "Sample document: (['hi', 'there'], 'greetings')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "dataset_path=\"data.json\"\n",
    "with open(dataset_path, 'r') as f:\n",
    "  data = json.load(f)\n",
    "def preprocess_data(dataset_path):\n",
    "  \"\"\"\n",
    "  Preprocesses chatbot intent data from a JSON file.\n",
    "\n",
    "  Args:\n",
    "      dataset_path (str): Path to the JSON file containing intents data.\n",
    "\n",
    "  Returns:\n",
    "      tuple: A tuple containing the following elements:\n",
    "          - words (list): List of all cleaned and lemmatized words.\n",
    "          - classes (list): List of unique intent tags.\n",
    "          - documents (list): List of tuples containing (cleaned message, intent tag).\n",
    "          - ignore_words (list): List of words to ignore during pre-processing (optional).\n",
    "  \"\"\"\n",
    "\n",
    "  words = []\n",
    "  classes = []\n",
    "  documents = []\n",
    "  ignore_words = ['?', '!']\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  # Load the JSON data\n",
    "  with open(dataset_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "  # Iterate through intents and patterns\n",
    "  for intent in data[\"intents\"]:\n",
    "    for pattern in intent['patterns']:\n",
    "      # Tokenize the pattern (word separation)\n",
    "      w = nltk.word_tokenize(pattern.lower())\n",
    "      words.extend(w)\n",
    "\n",
    "      # Add document (cleaned message, intent tag)\n",
    "      cleaned_pattern = [lemmatizer.lemmatize(word) for word in w if word not in ignore_words]\n",
    "      documents.append((cleaned_pattern, intent['tag']))\n",
    "\n",
    "      # Add classes (unique intent tags)\n",
    "      if intent['tag'] not in classes:\n",
    "        classes.append(intent['tag'])\n",
    "\n",
    "  # Sort and remove duplicates from words and classes\n",
    "  words = sorted(list(set(words)))\n",
    "  classes = sorted(list(set(classes)))\n",
    "\n",
    "  return words, classes, documents, ignore_words\n",
    "\n",
    "# Example usage\n",
    "dataset_path = \"data.json\"  # Replace with your actual dataset path\n",
    "words, classes, documents, ignore_words = preprocess_data(dataset_path)\n",
    "\n",
    "# Print some information (optional)\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Number of classes (intent tags):\", len(classes))\n",
    "print(\"Sample document:\", documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 documents\n",
      "27 classes ['advancement of ayurveda', 'arthritis', 'ayurbot', 'ayurveda and homeopathy', 'ayurvedic medication', 'body pain', 'cancer', 'cold', 'cough', 'diabetes', 'drugs and its uses', 'fever', 'goodbye', 'greetings', 'headache', 'heart disease', 'history', 'joints pain', 'pregnancy', 'red eyes', 'running nose', 'side effects of ayurveda', 'sour throat', 'stomachache', 'thanks', 'wet cough', 'why ayurveda']\n",
      "131 unique lemmatized words ['&', \"'s\", '(', ')', ',', '-', ':', 'a', 'about', 'advancement', 'advantage', 'all', 'and', 'anjana', 'application', 'are', 'aristab', 'arka', 'arthritis', 'asava', 'avaleha', 'ayurveda', 'ayurvedic', 'bad', 'better', 'between', 'bhasma', 'bodypain', 'bye', 'c', 'cancer', 'chuma', 'churna', 'churna/kashayam', 'cold', 'cough', 'd', 'describe', 'diabetes', 'difference', 'disadvantage', 'disease', 'drug', 'dry', 'e', 'effect', 'eye', 'f', 'for', 'g', 'ghrita', 'goodbye', 'guggulu', 'gutika', 'h', 'have', 'headache', 'heart', 'hello', 'hi', 'history', 'homeopathy', 'how', 'i', 'ingredient', 'is', 'it', 'j', 'joint', 'k', 'kalpana', 'khand', 'kshara', 'kvatha', 'l', 'later', 'lauha', 'lavana', 'lepa', 'local', 'm', 'mandura', 'medication', 'medicine', 'multi-ingredient', 'n', 'netrabindu', 'nose', 'o', 'of', 'or', 'origin', 'p', 'pain', 'pak', 'parpati', 'pishti', 'pregnancy', 'q', 'r', 'rasayoga', 'red', 'running', 's', 'sattva', 'see', 'side', 'single', 'sour', 'stomach', 'stomachache', 'sub', 't', 'taila', 'tell', 'thank', 'thanks', 'there', 'these', 'thorat', 'throat', 'us', 'varients', 'varti', 'vati', 'wet', 'which', 'who', 'why', 'you', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "print (len(documents), \"documents\")\n",
    "\n",
    "print (len(classes), \"classes\", classes)\n",
    "\n",
    "print (len(words), \"unique lemmatized words\", words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Feature extraction and model training\u001b[39;00m\n\u001b[1;32m     28\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[0;32m---> 29\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get words from documents\u001b[39;00m\n\u001b[1;32m     30\u001b[0m labels \u001b[38;5;241m=\u001b[39m [intent \u001b[38;5;28;01mfor\u001b[39;00m _, intent \u001b[38;5;129;01min\u001b[39;00m documents]  \u001b[38;5;66;03m# Get intent tags from documents\u001b[39;00m\n\u001b[1;32m     31\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "def predict_class(model, vectorizer, message):\n",
    "  \"\"\"\n",
    "  Predicts the intent class for a given user message.\n",
    "\n",
    "  Args:\n",
    "      model (sklearn.naive_bayes.MultinomialNB): Trained Naive Bayes model.\n",
    "      vectorizer (sklearn.feature_extraction.text.CountVectorizer): Fitted vectorizer.\n",
    "      message (str): User message to predict the intent for.\n",
    "\n",
    "  Returns:\n",
    "      str: The predicted intent class (tag).\n",
    "  \"\"\"\n",
    "\n",
    "  # Preprocess the message\n",
    "  words = nltk.word_tokenize(message.lower())\n",
    "  cleaned_pattern = [lemmatizer.lemmatize(word) for word in words if word not in ignore_words]\n",
    "  # Convert the message to a vector\n",
    "  new_vector = vectorizer.transform([cleaned_pattern])\n",
    "  # Predict the class\n",
    "  predicted_class = model.predict(new_vector)[0]\n",
    "  return predicted_class\n",
    "\n",
    "# Example usage\n",
    "dataset_path = \"data.json\"  # Replace with your actual dataset path\n",
    "words, classes, documents, ignore_words = preprocess_data(dataset_path)\n",
    "\n",
    "# Feature extraction and model training\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([doc[0] for doc in documents])  # Get words from documents\n",
    "labels = [intent for _, intent in documents]  # Get intent tags from documents\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "# Optional: Evaluate the model on the testing data (accuracy)\n",
    "\n",
    "# Chatbot interaction loop\n",
    "lemmatizer = WordNetLemmatizer()  # Needed for real-time message pre-processing\n",
    "\n",
    "while True:\n",
    "  message = input(\"You: \")\n",
    "  # Preprocess the message\n",
    "  words = nltk.word_tokenize(message.lower())\n",
    "  cleaned_pattern = [lemmatizer.lemmatize(word) for word in words if word not in ignore_words]\n",
    "  # Convert the message to a vector\n",
    "  new_vector = vectorizer.transform([cleaned_pattern])\n",
    "  # Predict the class\n",
    "  predicted_class = classifier.predict(new_vector)[0]\n",
    "\n",
    "  # Respond based on the predicted class (replace with your actual responses)\n",
    "  print(\"Bot:\", greetings[predicted_class] if predicted_class in greetings else \"Sorry, I don't understand.\")\n",
    "\n",
    "# Example greetings dictionary (replace with your responses for each intent)\n",
    "greetings = {\n",
    "  \"greeting\": \"Hi there!\",\n",
    "  \"goodbye\": \"Goodbye! Have a nice day.\",\n",
    "  # ... (add more greetings for different intents)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  # Using CountVectorizer for this example\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create a CountVectorizer (alternative: TF-IDFVectorizer)\n",
    "vectorizer = CountVectorizer()\n",
    "empty_docs = [doc for doc in documents if not doc[0]]  # Check if any document's word list (first element) is empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(empty_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if empty_docs:\n",
    "  print(\"Warning: Found empty documents after pre-processing:\", len(empty_docs))\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Attempt to fit the vectorizer\n",
    "try:\n",
    "  X = vectorizer.fit_transform([[word.lower() for word in doc[0]] for doc in documents])\n",
    "except ValueError as e:\n",
    "  if \"empty vocabulary\" in str(e):\n",
    "    print(\"Error: Empty vocabulary. Consider reviewing pre-processing or using TF-IDF.\")\n",
    "  else:\n",
    "    raise e \n",
    "# Separate features and labels\n",
    "labels = [doc[1] for doc in documents]  # Get intent tags using indexing\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)  # Split data 80% train, 20% test\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Compile the model (specifying loss function, optimizer, and metrics)\n",
    "classifier.compile(loss='multinomial_nb', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data (already done in fit)\n",
    "# classifier.fit(X_train, y_train)  # This line is already executed in the previous step\n",
    "\n",
    "# Evaluate the model on the testing data (optional)\n",
    "# results = classifier.evaluate(X_test, y_test)\n",
    "# print(\"Test Loss:\", results[0])\n",
    "# print(\"Test Accuracy:\", results[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
